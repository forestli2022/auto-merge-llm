n_trials: 10
seed: 0
num_hidden_layers: 16
per_block_dim: 32
reg_warmup_steps: 5
layers: 10
base_model: "meta-llama/Llama-3.2-1B"
models:
    - "ai-nexuz/llama-3.2-1b-instruct-fine-tuned"
    - "erbacher/llama3.2-1B-MATH"