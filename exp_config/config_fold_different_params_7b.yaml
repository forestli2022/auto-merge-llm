strategy: fold_different_params_7b
global_params:
  random_seed: 0
  cache_dir: "/rds/general/user/fl1123/ephemeral/.cache"
  # load_run_history: None
  output_path: "/rds/general/user/fl1123/home/code/auto-merge-llm/output/fold_different_params_7b"
  evaluation:
    in_memory: true
    vllm: true
    enforce_eager: true
    batch_size: 16
    torch_dtype: float16
    #num_fewshot: None
    #limit: 10  # Limit the number of examples per task (for debug). 
    device: "cuda"
    include_path: ""
    tasks:
      - task: piqa
        metric: "acc,none"
        test_split: train
      - task: wsc
        metric: "acc,none"
        test_split: train
      - task: commonsense_qa
        metric: "acc,none"
        test_split: train
      - task: mmlu
        metric: "acc,none"
        test_split: validation
    final_evaluation_tasks:
      - task: piqa
        metric: "acc,none"
        # test_split: validation

      - task: wsc
        metric: "acc,none"
        # test_split: validation

      - task: commonsense_qa
        metric: "acc,none"
        # test_split: validation

      - task: boolq
        metric: "acc,none"
        # test_split: validation

      - task: mmlu
        metric: "acc,none"
        # test_split: validation

      - task: race
        metric: "acc,none"
        # test_split: test

      - task: hellaswag
        metric: "acc,none"
        # test_split: validation

      # - task: xsum
      #   metric: "rouge1"
      #   # test_split: validation
strategies:
  normal_models: !include normal_models.yaml
  normal_slices: !include normal_slices.yaml
  lfs: !include lfs.yaml
  dis: !include dis.yaml
  # prune: !include prune_7b.yaml
  # fold: !include fold_7b.yaml
  # fold_merge_once: !include fold_merge_once_7b.yaml
  fold_different_params_7b: !include fold_7b.yaml