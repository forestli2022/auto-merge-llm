eta: 3 
n_workers: 2
n_trials: 500
random_init_points: 0
min_budget: 100  # minimum budget per trial
max_budget: 1000 # maximum budget per trial 1000 
total_budget: 0
num_hidden_layers: 40
layers: 30
base_model: "meta-llama/Llama-2-13b-hf"
models:
    - "WizardLMTeam/WizardLM-13B-V1.2"
    - "vanillaOVO/WizardMath-13B-V1.0"
    - "layoric/llama-2-13b-code-alpaca"
merging_method: 
  task_arithmetic:
    scaling_coefficient: 
      min: 0.0
      max: 1.0
  slerp:
    slerp_t:
      min: 0.0
      max: 1.0
  ties:
    param_value_mask_rate:
      min: 0.0
      max: 0.99
    scaling_coefficient:
      min: 0.0
      max: 1.0
  linear: 
    weights:
      min: 0.0
      max: 1.0